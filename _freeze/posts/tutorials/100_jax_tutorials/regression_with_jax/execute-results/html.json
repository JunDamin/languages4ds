{
  "hash": "1e4550e4a88720c6fb0961f2329f198e",
  "result": {
    "markdown": "---\ntitle: Jax를 사용해서 Regression을 그려 봅시다\ndate: 2022-08-22\ncategories:\n  - python\n  - jax\n  - regression\n  - tutorial\nimage: regression.png\n---\n\n### 설치하기\n\nJAX를 설치해 봅시다.\n설치를 할 때 먼저 jaxlib을 설치하고 그 버전에 맞는 jax를 설치합니다.\n순서가 바뀐 경우 잘 안되는 문제가 발생하곤 했습니다.\n\nlinux에서는 문제가 없는 것으로 보이나 windows에서 실행할 경우에는\n[jaxlib whl](https://whls.blob.core.windows.net/unstable/index.html)에서 jaxlib을 받아서 설치해야 합니다.\n\n그리고 그 파일 버전에 맞춰서 jax를 설치해 주어야 합니다.\n저 같은 경우에는 `jax` 0.3.7으로 설치했습니다.\n```bash\npip install jaxlib-0.3.7-cp310-none-win_amd64.whl\npip install jax==0.3.7\n```\n\n### 필요한 패키지를 불러옵니다\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom jax import numpy as jnp\nfrom jax import grad\nimport numpy as np\nfrom plotnine import *\nimport pandas as pd\nfrom tqdm import tqdm\n```\n:::\n\n\n간단한 모형을 만들어 봅니다. X와 y가 2차함수 형태로 결합된 경우를 생각해 봅니다.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nn = 100\nX = np.random.uniform(0, 3, size=n)\ny = 3 * np.power(X, 2) + np.random.normal(10, 3, size=n)\n\ndata = pd.DataFrame(zip(X, y), columns=[\"X\", \"y\"])\n(\n    ggplot(data)\n    + aes(\"X\", \"y\")\n    + geom_point()\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](regression_with_jax_files/figure-html/cell-3-output-1.png){}\n:::\n:::\n\n\n선형 모델을 먼저 생각해 봅니다.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nw = {\"a\": 0., \"b\": 0.}\n\n# set model\ndef model(w, X):\n    return w[\"a\"] * X + w[\"b\"]\n\n# set loss\ndef loss(w, model, X, y):\n    return jnp.power(model(w, X) - y, 2).sum()\n\n# grad loss\ndloss = grad(loss)\n```\n:::\n\n\n이제 경사하강법을 활용하여 w를 찾아봅니다.\n\n경사하강법은 말그대로 경사를 구해서 낮은 쪽으로 이동하게 하는 것입니다.\n\n기본적인 아이디어는 예측치와 관측값의 차이를 합치는 손실함수(loss function)을 구합니다. 그리고 파라미터를 손실이 줄어드는 방향(경사, 미분해서 보통 구합니다)으로 조금씩 옮겨가면서 최적의 값을 찾아 한발 한발 나아가는 방식입니다.\n\n수식으로 간단하게 표기해보자면\n$$Loss = \\sum_i f(w, data_i)$$\n로 정의하고 $\\sum_i f(w, data_i)$를 $w$로 미분해서 해당 미분값(경사)를 이용해서 낮추는 방향으로 파라미터를 바꿔가면서 찾아가는 방식입니다.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nrate = 0.0001\n\nlosses = []\nws = []\nfor i in tqdm(range(100)):\n    l = loss(w, model, X, y)\n    ws.append(w.copy())\n    losses.append(l)\n    dw = dloss(w, model, X, y)\n    for key in w.keys():\n        w[key] -= dw[key]*rate \n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/100 [00:00<?, ?it/s]WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\r  1%|          | 1/100 [00:01<02:12,  1.34s/it]\r 15%|█▌        | 15/100 [00:01<00:06, 14.13it/s]\r 39%|███▉      | 39/100 [00:01<00:01, 41.31it/s]\r 61%|██████    | 61/100 [00:01<00:00, 67.50it/s]\r 88%|████████▊ | 88/100 [00:01<00:00, 102.17it/s]\r100%|██████████| 100/100 [00:01<00:00, 55.66it/s]\n```\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# overlay plots\nresult_df = pd.DataFrame(zip(X, np.array(model(w, X))), columns=[\"X\", \"f\"])\n(\n    ggplot(data=data) +\n    aes(\"X\", \"y\") +\n    geom_point() +\n    geom_smooth(method=\"lm\") +\n    geom_line(data=result_df, mapping=aes(\"X\", \"f\"),  color=\"#ff1234\")\n    \n)\n```\n\n::: {.cell-output .cell-output-display}\n![](regression_with_jax_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndfs = [pd.DataFrame(zip(map(int, np.ones_like(X)*i), X, np.array(model(ws[i], X))), columns=[\"i\", \"X\", \"f\"]) for i in range(0, 50, 5)]\ndf = pd.concat(dfs)\n```\n:::\n\n\n처음에는 많이 차이나지만 점점 해석적으로 계산한 선형 회귀 값과 유사해지는 것을 볼 수 있습니다.\n\n이 경사하강법의 장점은 손실함수를 정의 할 수만 있다면 적용할 수 있어 유연하게 많은 곳에 적용할 수 있습니다.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\np = (\n    ggplot(data=df) +\n    aes(x=\"X\", y=\"f\") +\n    geom_point(data=data, mapping=aes(\"X\", \"y\")) +\n    geom_smooth(data=data, method=\"lm\", mapping=aes(\"X\", \"y\"), color=\"yellow\") +\n    geom_line(color=\"red\", size=1) +\n    facet_wrap(\"i\")\n)\np\n```\n\n::: {.cell-output .cell-output-display}\n![](regression_with_jax_files/figure-html/cell-8-output-1.png){}\n:::\n:::\n\n\n하나의 그래프에 겹쳐서 표현하면 아래와 같은 그래프가 됩니다.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\np = (ggplot() +\n    geom_point(data=data, mapping=aes(\"X\", \"y\")) +\n    geom_smooth(data=data, method=\"lm\", mapping=aes(\"X\", \"y\"), color=\"yellow\")\n)\n\nfor df in dfs:\n    p += geom_line(data=df, mapping=aes(x=\"X\", y=\"f\", color=\"i\"))\n\n\np\n```\n\n::: {.cell-output .cell-output-display}\n![](regression_with_jax_files/figure-html/cell-9-output-1.png){}\n:::\n:::\n\n\n",
    "supporting": [
      "regression_with_jax_files"
    ],
    "filters": [],
    "includes": {}
  }
}